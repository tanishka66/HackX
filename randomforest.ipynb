{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c7799f-7f05-4972-95e3-186ef4c35b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'drop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 225\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rf_clf, y_pred\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Assuming you have a function that generates or loads inputs_and_labels\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m inputs_and_labels \u001b[38;5;241m=\u001b[39m \u001b[43mget_samples_from_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHackX/Datasets/All_EEG_files.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbandpass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m model, predictions \u001b[38;5;241m=\u001b[39m classify_attention(inputs_and_labels)\n",
      "Cell \u001b[0;32mIn[3], line 38\u001b[0m, in \u001b[0;36mget_samples_from_test\u001b[0;34m(df, slider, sample_size, bandpass)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_samples_from_test\u001b[39m(df, slider, sample_size, bandpass):\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Method for creating samples within a dataset\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m    :df: test that is being sampled\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m AdjustedUnix\u001b[39m\u001b[38;5;124m\"\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#remove unnecessary columns\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     Sample \u001b[38;5;241m=\u001b[39m namedtuple(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSample\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minputs effort attention interest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m     sampled_tests \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'drop'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tflow\n",
    "\n",
    "\n",
    "def clean_training_files():\n",
    "\n",
    "    tf = pd.read_csv(\"HackX/Datasets/Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    tf = tf[(tf['user'] != 21) & (tf['user'] != 16)] # remove users 16 and 21 as they only have 2 and 1 test respectively\n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "\n",
    "\n",
    "def get_samples_from_test(df, slider, sample_size, bandpass):\n",
    "    \"\"\"\n",
    "    Method for creating samples within a dataset\n",
    "    :df: test that is being sampled\n",
    "    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\n",
    "    \"\"\"\n",
    "    df = df.drop([\"Timestamp\", \" AdjustedUnix\"], axis=1) #remove unnecessary columns\n",
    "    Sample = namedtuple('Sample', 'inputs effort attention interest')\n",
    "    sampled_tests = []\n",
    "    \n",
    "    \n",
    "    # Group by paragraph and add each paragraph into an array\n",
    "    paragraphs = df.groupby('para') \n",
    "    paragraphs = [paragraphs.get_group(x) for x in paragraphs.groups]\n",
    "    \n",
    "    incorrect_length = 0\n",
    "    # Loop trough each paragraph to create samples\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        if not len(para) > sample_size: # check the length of paragraph if it is bigger than the sample size\n",
    "            continue\n",
    "            \n",
    "        new_sample_length = len(para[0:sample_size])\n",
    "        counter = 0\n",
    "        while  new_sample_length >= sample_size: # this checks that the new sample is at least greater than or equal to sample size\n",
    "            \"\"\"\n",
    "            **Sliding window algorithm**\n",
    "            - Create new samples based on sample size and iterate using the slider size for size of overlap\n",
    "            - Create separate values for inputs, effort, attention, interest to add to a tuple\n",
    "            \"\"\"\n",
    "            new_sample = para[counter : counter + sample_size] #gets new sample based on the counter and sample size\n",
    "            new_sample_length = len(new_sample)\n",
    "            \n",
    "            #checks new_sample length\n",
    "            if new_sample_length == sample_size:\n",
    "                # Extract the sample specific data and apply band pass filtering if true\n",
    "                if bandpass == True:\n",
    "#                     _, inputs = chebyBandpassFilter(array(new_sample.iloc[:, :8]), [0.05, 0.1, 40, 42])\n",
    "                    inputs = filter_sample(array(new_sample.iloc[:, :8]))\n",
    "                else:\n",
    "                    inputs = array(new_sample.iloc[:, :8])\n",
    "                \n",
    "                effort, attention, interest = new_sample[[\"effort\", \"attention\", \"interest\"]].T.values\n",
    "                sampled_tests.append(Sample(inputs, int(max(effort)), int(max(attention)), int(max(interest))))\n",
    "            else:\n",
    "                incorrect_length += 1\n",
    "                continue             \n",
    "         \n",
    "            # increase by slider\n",
    "            counter += slider\n",
    "        \n",
    "        \n",
    "    sampled_tests_df = pd.DataFrame(sampled_tests) #sampled list data frame\n",
    "    print(\"This sampled test has {0} samples\".format(len(sampled_tests_df)))\n",
    "    inputs_and_labels = {}\n",
    "    \n",
    "    inputs_list = sampled_tests_df['inputs'].values\n",
    "    inputs_list = np.rollaxis(np.dstack(inputs_list),-1) #combine all the inputs into 3D array\n",
    "    inputs_and_labels['inputs'] = inputs_list # add inputs into dictionary\n",
    "    labels = [\"effort\", \"attention\", \"interest\"]\n",
    "    for label in labels: \n",
    "        inputs_and_labels[label] = array(sampled_tests_df[label].values) #add lables to dictionary\n",
    "    \n",
    "\n",
    "    print(inputs_and_labels['inputs'])\n",
    "    return inputs_and_labels  \n",
    "\n",
    "\n",
    "def generate_all_samples_or_tests(slider, sample_size, agg, bandpass):\n",
    "    \"\"\"\n",
    "    Method for generating samples for all the tests with a default sample size of 60 and slide of 60. \n",
    "    Combines all tests of a user into a tuple consisting of inputs, attention, interest, effort. \n",
    "    Saves all tests in a dictionary\n",
    "    :slider:\n",
    "    :sample_size:\n",
    "    \"\"\"\n",
    "    clean_tf = pd.read_csv(\"clean_trainingfiles.csv\")\n",
    "    users = set(clean_tf['user'])\n",
    "    user_tests = {}\n",
    "    \n",
    " \n",
    "    for user in users:\n",
    "        \n",
    "        #store all of user's test in a dictionary\n",
    "        user_test_paths = array(clean_tf[clean_tf['user'] == user][\"path\"])\n",
    "    \n",
    "\n",
    "        file = \"annotated_EEG.csv\"\n",
    "\n",
    "        # Loop through all the tests, generate samples and then append them to an array in the dictionary\n",
    "        inputs_and_labels = {\"inputs\":[], \"attention\":[], \"effort\":[], \"interest\":[]}\n",
    "        for test_path in user_test_paths:\n",
    "            print(\"Processing user {0} , test {1}\".format(extract_user_number(test_path), extract_test_number(test_path)))\n",
    "            test_file = test_path + \"/\" + file\n",
    "            test_dataset = pd.read_csv(test_file)\n",
    "            if len(test_dataset) == 0:\n",
    "                continue\n",
    "        #convert the test into windowed format with samples     \n",
    "            sampled_test_dataset = get_samples_from_test(test_dataset, slider, sample_size, bandpass)\n",
    "        #add all the tests to the dictionary, inputs and labels\n",
    "            for key in inputs_and_labels:\n",
    "                inputs_and_labels[key].append(sampled_test_dataset[key])\n",
    "                \n",
    "        if agg == True: \n",
    "            # loop through he dictionary and concatenate the list\n",
    "            for key in inputs_and_labels:\n",
    "                inputs_and_labels[key] = np.concatenate(inputs_and_labels[key], axis=0)\n",
    "                print(\"Shape of {0}: {1}\".format(key,inputs_and_labels[key].shape ))\n",
    "        \n",
    "        user_tests[user] = inputs_and_labels\n",
    "        print(user_tests.keys())\n",
    "\n",
    " \n",
    "    print(\"Adding dictionary...\")\n",
    "    return user_tests\n",
    "\n",
    "\n",
    "\n",
    "def save_datasets(window_size, slider, bandpass):\n",
    "    all_tests_agg = generate_all_samples_or_tests(slider,window_size,agg=True, bandpass=bandpass)\n",
    "    all_tests_no_agg = generate_all_samples_or_tests(slider,window_size,agg=False, bandpass=bandpass)\n",
    "    saved_file_agg = \"HackX/Datasets/saved_user_and_test_data/all_users_sampled_{0}_window_annotated_EEG_agg_bandpass_{1}_slider_{2}.pickle\".format(window_size,bandpass, slider)\n",
    "    saved_file_no_agg = \"HackX/Datasets/saved_user_and_test_data/all_users_sampled_{0}_window_annotated_EEG_no_agg_bandpass_{1}_slider_{2}.pickle\".format(window_size,bandpass, slider)\n",
    "    save_file(saved_file_agg, all_tests_agg)\n",
    "    save_file(saved_file_no_agg, all_tests_no_agg)\n",
    "\n",
    "\n",
    "\n",
    "def combine_test_per_user(agg=False):\n",
    "    \"\"\"\n",
    "    Combines all tests per user without sampling\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"clean_trainingfiles.csv\")\n",
    "    users = set(array(df['user']))\n",
    "    all_users = {}\n",
    "    \n",
    "    file = \"annotated_EEG.csv\"\n",
    "    for user in users:\n",
    "        test_paths = array(df[df['user'] == user][\"path\"])\n",
    "        test_list = []\n",
    "        for test_path in test_paths:\n",
    "            test_file = test_path + \"/\" + file\n",
    "            test_dataset = pd.read_csv(test_file)\n",
    "            test_list.append(test_dataset)\n",
    "            \n",
    "        if agg == True: test_list  = pd.concat(test_list)\n",
    "            \n",
    "        all_users[user] = test_list\n",
    "        print(\"Processed user {0}:\\tDataframe size: {1}\".format(user, len(test_list)))\n",
    "        \n",
    "    print(\"Saving dictionary...\")\n",
    "    saved_file = \"HackX/Datasets/saved_user_and_test_data/all_tests_EEG_{0}.pickle\".format(agg)\n",
    "    with open(saved_file, 'wb') as handle:            \n",
    "        pickle.dump(all_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return all_users \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the processed inputs_and_labels from your previous functions\n",
    "def classify_attention(inputs_and_labels):\n",
    "    \"\"\"\n",
    "    This function will classify attention levels into 3 categories using RandomForestClassifier.\n",
    "    :inputs_and_labels: dictionary containing 'inputs' and 'attention' labels\n",
    "    \"\"\"\n",
    "    # Extract inputs and labels\n",
    "    X = inputs_and_labels['inputs']\n",
    "    y = inputs_and_labels['attention']\n",
    "    \n",
    "    # Convert attention levels into 3 categories (low, medium, high)\n",
    "    attention_labels = pd.cut(y, bins=3, labels=[0, 1, 2])  # 0: low, 1: medium, 2: high\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, attention_labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Initialize and train Random Forest Classifier\n",
    "    rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = rf_clf.predict(X_test)\n",
    "    \n",
    "    # Output evaluation metrics\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return rf_clf, y_pred\n",
    "\n",
    "# Assuming you have a function that generates or loads inputs_and_labels\n",
    "inputs_and_labels = get_samples_from_test(df=\"HackX/Datasets/All_EEG_files.csv\", slider=60, sample_size=60, bandpass=False)\n",
    "model, predictions = classify_attention(inputs_and_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c8c6a-4f16-499e-aa25-f78e0c0191da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
