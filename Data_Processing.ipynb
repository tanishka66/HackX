{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0323fe4-81f0-4ff4-ba87-ed6bccec9941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75bf536a-6681-4b1e-82c2-c1d51d667229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflow.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64a7e08d-e9e7-47a2-b6a2-954f5be84235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_files():\n",
    "\n",
    "    tf = pd.read_csv(\"Trainingfiles.csv\")\n",
    "    print(\"Initial length of training files: {}\".format(len(tf)))\n",
    "    \n",
    "    zero_final = tf['final_rows'] == 0 # remove the files with zero final row\n",
    "    empty_json = tf['no_para'] == 0 # remove files with empty JSON's\n",
    "    four_para = tf['no_para'] == 4 # remove files with only four paragraphs\n",
    "    indistinct_timestamp = tf['are_timestamps_distinct'] == False # remove files with indistinct timestamps\n",
    "    files_rm = [zero_final, empty_json, four_para, indistinct_timestamp] # store them all in a list\n",
    "    for file_rm in files_rm:\n",
    "        tf = tf.drop(tf[file_rm].index) # drop files sequentially\n",
    "    \n",
    "    tf = tf[(tf['user'] != 21) & (tf['user'] != 16)] # remove users 16 and 21 as they only have 2 and 1 test respectively\n",
    "    print(\"Final length of training files: {}\".format(len(tf)))\n",
    "    tf.to_csv(\"clean_trainingfiles.csv\", index=False)\n",
    "    return tf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "082475fe-9f1d-483f-aa2a-bcc3e4b882e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_test(df, slider, sample_size, bandpass):\n",
    "    \"\"\"\n",
    "    Method for creating samples within a dataset\n",
    "    :df: test that is being sampled\n",
    "    :slider: the amount by which the window slides during sampling. The lower the number, the more samples.\n",
    "    \"\"\"\n",
    "    df = df.drop([\"Timestamp\", \" AdjustedUnix\"], axis=1) #remove unnecessary columns\n",
    "    Sample = namedtuple('Sample', 'inputs effort attention interest')\n",
    "    sampled_tests = []\n",
    "    \n",
    "    \n",
    "    # Group by paragraph and add each paragraph into an array\n",
    "    paragraphs = df.groupby('para') \n",
    "    paragraphs = [paragraphs.get_group(x) for x in paragraphs.groups]\n",
    "    \n",
    "    incorrect_length = 0\n",
    "    # Loop trough each paragraph to create samples\n",
    "    for para in paragraphs:\n",
    "        \n",
    "        if not len(para) > sample_size: # check the length of paragraph if it is bigger than the sample size\n",
    "            continue\n",
    "            \n",
    "        new_sample_length = len(para[0:sample_size])\n",
    "        counter = 0\n",
    "        while  new_sample_length >= sample_size: # this checks that the new sample is at least greater than or equal to sample size\n",
    "            \"\"\"\n",
    "            **Sliding window algorithm**\n",
    "            - Create new samples based on sample size and iterate using the slider size for size of overlap\n",
    "            - Create separate values for inputs, effort, attention, interest to add to a tuple\n",
    "            \"\"\"\n",
    "            new_sample = para[counter : counter + sample_size] #gets new sample based on the counter and sample size\n",
    "            new_sample_length = len(new_sample)\n",
    "            \n",
    "            #checks new_sample length\n",
    "            if new_sample_length == sample_size:\n",
    "                # Extract the sample specific data and apply band pass filtering if true\n",
    "                if bandpass == True:\n",
    "#                     _, inputs = chebyBandpassFilter(array(new_sample.iloc[:, :8]), [0.05, 0.1, 40, 42])\n",
    "                    inputs = filter_sample(array(new_sample.iloc[:, :8]))\n",
    "                else:\n",
    "                    inputs = array(new_sample.iloc[:, :8])\n",
    "                \n",
    "                effort, attention, interest = new_sample[[\"effort\", \"attention\", \"interest\"]].T.values\n",
    "                sampled_tests.append(Sample(inputs, int(max(effort)), int(max(attention)), int(max(interest))))\n",
    "            else:\n",
    "                incorrect_length += 1\n",
    "                continue             \n",
    "         \n",
    "            # increase by slider\n",
    "            counter += slider\n",
    "        \n",
    "        \n",
    "    sampled_tests_df = pd.DataFrame(sampled_tests) #sampled list data frame\n",
    "    print(\"This sampled test has {0} samples\".format(len(sampled_tests_df)))\n",
    "    inputs_and_labels = {}\n",
    "    \n",
    "    inputs_list = sampled_tests_df['inputs'].values\n",
    "    inputs_list = np.rollaxis(np.dstack(inputs_list),-1) #combine all the inputs into 3D array\n",
    "    inputs_and_labels['inputs'] = inputs_list # add inputs into dictionary\n",
    "    labels = [\"effort\", \"attention\", \"interest\"]\n",
    "    for label in labels: \n",
    "        inputs_and_labels[label] = array(sampled_tests_df[label].values) #add lables to dictionary\n",
    "    \n",
    "\n",
    "    print(inputs_and_labels['inputs'])\n",
    "    return inputs_and_labels  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "763e247d-96fd-45b7-b1e3-ba6605de6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_samples_or_tests(slider, sample_size, agg, bandpass):\n",
    "    \"\"\"\n",
    "    Method for generating samples for all the tests with a default sample size of 60 and slide of 60. \n",
    "    Combines all tests of a user into a tuple consisting of inputs, attention, interest, effort. \n",
    "    Saves all tests in a dictionary\n",
    "    :slider:\n",
    "    :sample_size:\n",
    "    \"\"\"\n",
    "    clean_tf = pd.read_csv(\"clean_trainingfiles.csv\")\n",
    "    users = set(clean_tf['user'])\n",
    "    user_tests = {}\n",
    "    \n",
    " \n",
    "    for user in users:\n",
    "        \n",
    "        #store all of user's test in a dictionary\n",
    "        user_test_paths = array(clean_tf[clean_tf['user'] == user][\"path\"])\n",
    "    \n",
    "\n",
    "        file = \"annotated_EEG.csv\"\n",
    "\n",
    "        # Loop through all the tests, generate samples and then append them to an array in the dictionary\n",
    "        inputs_and_labels = {\"inputs\":[], \"attention\":[], \"effort\":[], \"interest\":[]}\n",
    "        for test_path in user_test_paths:\n",
    "            print(\"Processing user {0} , test {1}\".format(extract_user_number(test_path), extract_test_number(test_path)))\n",
    "            test_file = test_path + \"/\" + file\n",
    "            test_dataset = pd.read_csv(test_file)\n",
    "            if len(test_dataset) == 0:\n",
    "                continue\n",
    "        #convert the test into windowed format with samples     \n",
    "            sampled_test_dataset = get_samples_from_test(test_dataset, slider, sample_size, bandpass)\n",
    "        #add all the tests to the dictionary, inputs and labels\n",
    "            for key in inputs_and_labels:\n",
    "                inputs_and_labels[key].append(sampled_test_dataset[key])\n",
    "                \n",
    "        if agg == True: \n",
    "            # loop through he dictionary and concatenate the list\n",
    "            for key in inputs_and_labels:\n",
    "                inputs_and_labels[key] = np.concatenate(inputs_and_labels[key], axis=0)\n",
    "                print(\"Shape of {0}: {1}\".format(key,inputs_and_labels[key].shape ))\n",
    "        \n",
    "        user_tests[user] = inputs_and_labels\n",
    "        print(user_tests.keys())\n",
    "\n",
    " \n",
    "    print(\"Adding dictionary...\")\n",
    "    return user_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0380fe0a-4ad7-455b-97ef-fbcb9e5541fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_datasets(window_size, slider, bandpass):\n",
    "    all_tests_agg = generate_all_samples_or_tests(slider,window_size,agg=True, bandpass=bandpass)\n",
    "    all_tests_no_agg = generate_all_samples_or_tests(slider,window_size,agg=False, bandpass=bandpass)\n",
    "    saved_file_agg = \"HackX/Datasets/saved_user_and_test_data/all_users_sampled_{0}_window_annotated_EEG_agg_bandpass_{1}_slider_{2}.pickle\".format(window_size,bandpass, slider)\n",
    "    saved_file_no_agg = \"HackX/Datasets/saved_user_and_test_data/all_users_sampled_{0}_window_annotated_EEG_no_agg_bandpass_{1}_slider_{2}.pickle\".format(window_size,bandpass, slider)\n",
    "    save_file(saved_file_agg, all_tests_agg)\n",
    "    save_file(saved_file_no_agg, all_tests_no_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "830e9481-9293-4733-bd11-a4fa07c051a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_test_per_user(agg=False):\n",
    "    \"\"\"\n",
    "    Combines all tests per user without sampling\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"clean_trainingfiles.csv\")\n",
    "    users = set(array(df['user']))\n",
    "    all_users = {}\n",
    "    \n",
    "    file = \"annotated_EEG.csv\"\n",
    "    for user in users:\n",
    "        test_paths = array(df[df['user'] == user][\"path\"])\n",
    "        test_list = []\n",
    "        for test_path in test_paths:\n",
    "            test_file = test_path + \"/\" + file\n",
    "            test_dataset = pd.read_csv(test_file)\n",
    "            test_list.append(test_dataset)\n",
    "            \n",
    "        if agg == True: test_list  = pd.concat(test_list)\n",
    "            \n",
    "        all_users[user] = test_list\n",
    "        print(\"Processed user {0}:\\tDataframe size: {1}\".format(user, len(test_list)))\n",
    "        \n",
    "    print(\"Saving dictionary...\")\n",
    "    saved_file = \"HackX/Datasets/saved_user_and_test_data/all_tests_EEG_{0}.pickle\".format(agg)\n",
    "    with open(saved_file, 'wb') as handle:            \n",
    "        pickle.dump(all_users, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return all_users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fed0f5-949c-4d02-a37a-70912d075248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc940d-2a90-4d8c-bb38-d8da259f5e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
